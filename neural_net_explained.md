# ğŸ§  Neural Networks From Scratch - Full Math & Intuition Explained

## ğŸ“‘ Table of Contents
- [Why Neural Networks](#-why-neural-networks)
- [Network Architecture](#-network-architecture)
- [Forward Propagation](#-forward-propagation)
- [Backpropagation](#-backpropagation)
- [Weight Updates](#-weight-updates)
- [Conclusion & Next Steps](#-conclusion--next-steps)


## ğŸ¤” Why Neural Networks?

In the world of machine learning, traditional algorithms like logistic regression or decision trees are often limited by their **inability to capture complex patterns** in data.

But what if:
- The decision boundary isnâ€™t linear?
- Youâ€™re working with raw pixels from images?
- You want a model that can **learn features automatically**?

This is where **Neural Networks (NNs)** shine.

### ğŸ” Core Idea

A neural network is a **function approximator**.  
It tries to learn a mapping:
f(X) -> Y
...where \( X \) could be image pixels and \( Y \) could be class labels (like digits 0â€“9).

Rather than manually crafting features (as in classical ML), NNs **learn the representations directly from data** using a process called **backpropagation**.

---

## ğŸ§± Why Build One *From Scratch*?

Most people use frameworks like PyTorch or TensorFlow. But when you build one from the ground up:
- You gain a **clear understanding of how backpropagation works**
- You learn what **each layer actually does under the hood**
- You build **strong debugging and intuition skills**

This project uses only **NumPy** to build and train a neural network on the **MNIST digit dataset** â€” a classic problem in image classification.

> âœ… Goal: Learn how to implement and train a neural network by hand â€” one matrix at a time.

---

## ğŸ“¦ Dataset Used: MNIST

- **28 Ã— 28 grayscale images** of digits 0 through 9
- **60,000 training samples**, **10,000 test samples**
- Each image is flattened into a **784-dimensional vector**

---

## ğŸ§± Network Architecture

### ğŸ“ Network Layout

We use a **simple 3-layer architecture** for classifying handwritten digits:

- **Input Layer**: 784 neurons (28x28 pixels, flattened grayscale image)
- **Hidden Layer**: 128 neurons with ReLU activation
- **Output Layer**: 10 neurons with Softmax activation (for 10 digit classes)

---

### ğŸ”„ Forward Propagation: Step by Step

Forward propagation is the process of passing input data through the layers to get the prediction.

Let:
- \( X \): Input matrix of shape (batch_size Ã— 784)
- \( W1, b1 \): Weights and biases of layer 1
- \( A1 \): Activation output of hidden layer
- \( W2, b2 \): Weights and biases of layer 2
- \( A2 \): Final output probabilities

---

### ğŸ”¹ Step 1: Linear Transformation (Input â†’ Hidden)

`Zâ‚ = X Â· Wâ‚ + bâ‚`

- X:     Matrix of shape (batch_size x 784)  
- Wâ‚:    Matrix of shape (784 x 128) 
- bâ‚:    Matrix of shape (1 x 128)  
- Zâ‚:    Matrix of shape (batch_size x 128)


---

### ğŸ”¹ Step 2: Apply Activation (ReLU)

`Aâ‚ = ReLU(Zâ‚) = max(0, Zâ‚)`

- This introduces **non-linearity**, allowing the network to learn complex functions.

---

### ğŸ”¹ Step 3: Linear Transformation (Hidden â†’ Output)

`Zâ‚‚ = Aâ‚ Â· Wâ‚‚ + bâ‚‚`

- Aâ‚: Matrix of shape (batch_size Ã— 128)  
- Wâ‚‚: Matrix of shape (128 Ã— 10)  
- bâ‚‚: Matrix of shape (1 Ã— 10)  
- Zâ‚‚: Matrix of shape (batch_size Ã— 10)

---

### ğŸ”¹ Step 4: Apply Activation (Softmax)

`Aâ‚‚ = Softmax(Zâ‚‚)`

- Converts raw scores into a **probability distribution** over 10 classes  
- Aâ‚‚: Matrix of shape (batch_size Ã— 10), where each row sums to 1

**Softmax formula**:  
`softmax(záµ¢) = e^(záµ¢) / Î£â±¼ e^(zâ±¼)`

---

### ğŸ’¡ Why ReLU and Softmax?

- **ReLU** (Rectified Linear Unit):
  - Simple and fast.
  - Prevents vanishing gradients (compared to sigmoid/tanh).
- **Softmax**:
  - Normalizes outputs to probabilities.
  - Used for multi-class classification.

---

### ğŸ§ª Summary of Forward Flow

```text
Input X (784 features)
     â†“
Linear: Zâ‚ = XWâ‚ + bâ‚
     â†“
Activation: Aâ‚ = ReLU(Zâ‚)
     â†“
Linear: Zâ‚‚ = Aâ‚Wâ‚‚ + bâ‚‚
     â†“
Activation: Aâ‚‚ = Softmax(Zâ‚‚)
     â†“
Output: Probability for each digit class (0â€“9)
```

---

## ğŸ” Backpropagation

### â“ What is Backpropagation?

Backpropagation is the **learning algorithm** used to train neural networks.

It's how the network **calculates the gradient** (i.e., the direction and strength of change needed) of its loss function with respect to its parameters (weights and biases), so it can update them and **minimize the error**.

---

### ğŸ¤” Why Do We Need It?

In forward propagation, we compute outputs.

But during training, we also have the **true labels** â€” and we want our network's prediction to match them.

To do that, we:
1. Compute the error (loss)
2. **Propagate that error backward** to figure out:
   - Which weights contributed most to the error
   - How much to adjust each weight to reduce that error

This is done using **the chain rule of calculus** â€” systematically computing gradients layer-by-layer, starting from the output and moving back to the input.

---

## ğŸ§® How Does It Work? (Step-by-Step)

Weâ€™ll assume we already did a **forward pass**, so we know:
- `Aâ‚‚`: Output from Softmax
- `Y`: True labels (one-hot encoded)
- Intermediate values: `Zâ‚`, `Aâ‚`, `Zâ‚‚`

Letâ€™s derive the gradients for each parameter.

---

### ğŸ”¹ Step 1: Gradient of Loss w.r.t. Output (Softmax + Cross-Entropy)

**Loss Function (Categorical Cross-Entropy):**

L = - Î£áµ¢ ( yáµ¢ * log(aâ‚‚áµ¢) )

Where:
- yáµ¢ is the true label (1 for correct class, else 0)
- aâ‚‚áµ¢ is the predicted probability for class i (from softmax)


**Gradient of loss w.r.t. Zâ‚‚**:

`dZâ‚‚ = Aâ‚‚ - Y`


> âœ… **Why?** For Softmax + Cross-Entropy combined, this is the elegant result of their derivative â€” simplifies backprop significantly.

---

### ğŸ”¹ Step 2: Gradients for Output Layer Parameters (Wâ‚‚, bâ‚‚)

Using `dZâ‚‚`:

- `dWâ‚‚ = (Aâ‚áµ€ Â· dZâ‚‚) / m`  
- `dbâ‚‚ = sum(dZâ‚‚) / m`

Where:
- `m` = batch size
- `dWâ‚‚` is the gradient of the loss w.r.t. the weights connecting hidden â†’ output
- `dbâ‚‚` is the gradient for the output bias

---

### ğŸ”¹ Step 3: Backprop to Hidden Layer (dZâ‚)

We now propagate error from output back to hidden:

- `dZâ‚ = (dZâ‚‚ Â· Wâ‚‚áµ€) * ReLU'(Zâ‚)`

> âœ… **Why the derivative of ReLU?**  
Because ReLU "kills" negative values â€” its gradient is 0 for `z â‰¤ 0`, and 1 for `z > 0`.

---

### ğŸ”¹ Step 4: Gradients for Hidden Layer Parameters (Wâ‚, bâ‚)

Now that we have `dZâ‚`, we can calculate:

- `dWâ‚ = (Xáµ€ Â· dZâ‚) / m`  
- `dbâ‚ = sum(dZâ‚) / m`

These gradients tell us how to update the input â†’ hidden layer weights and biases.

---

### ğŸ“¦ Summary of Backpropagation Flow

```text
Output Layer:
  dZâ‚‚ = Aâ‚‚ - Y
  dWâ‚‚ = Aâ‚áµ€ Â· dZâ‚‚ / m
  dbâ‚‚ = sum(dZâ‚‚) / m

Hidden Layer:
  dZâ‚ = (dZâ‚‚ Â· Wâ‚‚áµ€) * ReLU'(Zâ‚)
  dWâ‚ = Xáµ€ Â· dZâ‚ / m
  dbâ‚ = sum(dZâ‚) / m
```

---

## ğŸ”§ Part 4: Updating Weights (Gradient Descent)

### â“ What is Weight Update?

This is the **final step** of a training iteration â€” where the neural network actually learns.

After computing the gradients using backpropagation, we **update the weights and biases** so that the **loss decreases** in the next round.

This process is done using an optimization algorithm â€” in our case: **Gradient Descent**.

---

### ğŸ¤” Why Gradient Descent?

Gradient Descent is the most common and intuitive optimizer:

- It updates parameters by taking a **small step in the direction of negative gradient**
- Think of it as **"descending down a hill"** (loss landscape) to reach a minimum

---

### ğŸ§  How Does Gradient Descent Work?

We update weights and biases using this rule:

```text
parameter = parameter - learning_rate * gradient

Wâ‚ = Wâ‚ - lr Â· dWâ‚  
bâ‚ = bâ‚ - lr Â· dbâ‚  
Wâ‚‚ = Wâ‚‚ - lr Â· dWâ‚‚  
bâ‚‚ = bâ‚‚ - lr Â· dbâ‚‚
```

---

### Forward â†’ Compute Loss â†’ Backpropagation â†’ Update Weights â†’ Repeat



