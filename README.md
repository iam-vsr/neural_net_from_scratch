# ğŸ§  Neural Network From Scratch (NumPy Only)

> Build, train, and evaluate a neural network from the ground up â€” no frameworks, just pure NumPy and math.

---

## ğŸ“Œ Project Overview

This project walks through the step-by-step process of building a **fully connected feedforward neural network from scratch**, using **only NumPy**.

We train it on the classic **MNIST handwritten digit** dataset to classify digits (0â€“9), and break down:

- Matrix-based forward propagation
- Activation functions (ReLU, Softmax)
- Manual backpropagation using chain rule
- Gradient descent-based weight updates
- One-hot encoding and prediction logic

---

## ğŸ¯ Goals

- Understand how neural networks learn under the hood  
- Derive gradients and update rules from scratch  
- Train a model without relying on libraries like PyTorch or TensorFlow

---

## ğŸ§± Architecture

| Layer         | Size              | Activation |
|---------------|-------------------|------------|
| Input Layer   | 784 (28x28 pixels) | -          |
| Hidden Layer  | 128 neurons        | ReLU       |
| Output Layer  | 10 classes         | Softmax    |

---

ğŸ“˜ Read the full breakdown (math and intuition):  
[**neural_net_explained.md**](./neural_net_explained.md)

---

## ğŸ“¦ Dataset Used

**MNIST** â€” 28x28 grayscale images of digits  
- 60,000 training images  
- 10,000 testing images  
- One-hot encoded labels

---

## ğŸ“ˆ Results

- ğŸ“Š **Final Test Accuracy**: ~`84.68%`  
- ğŸƒâ€â™‚ï¸ Training runs for `500 epochs`  
- ğŸ§ª Optimized using plain gradient descent

---

## ğŸš€ Getting Started

No setup needed!

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iam-vsr/neural_net_from_scratch/blob/main/NN_from_scratch.ipynb)

Then click: `Runtime > Run all`

---


## ğŸ¤ Connect With Me
 
ğŸ”— [LinkedIn](https://www.linkedin.com/in/vansh-verma-v) â€¢ ğŸ“‚ [GitHub](https://www.github.com/iam-vsr)

---






